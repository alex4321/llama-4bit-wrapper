{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from llama_wrapper.core import import_llama"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama-wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a wrapper for [https://github.com/johnsmith0031/alpaca_lora_4bit](https://github.com/johnsmith0031/alpaca_lora_4bit) which allows you to do all the monkeypatching of the library without writing a bunch of stuff like next:\n",
    "\n",
    "```python\n",
    "# Early load config to replace attn if needed\n",
    "from arg_parser import get_config\n",
    "ft_config = get_config()\n",
    "\n",
    "from monkeypatch.peft_tuners_lora_monkey_patch import replace_peft_model_with_gptq_lora_model\n",
    "replace_peft_model_with_gptq_lora_model()\n",
    "\n",
    "if ft_config.flash_attention:\n",
    "    from monkeypatch.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n",
    "    replace_llama_attn_with_flash_attn()\n",
    "elif ft_config.xformers:\n",
    "    from monkeypatch.llama_attn_hijack_xformers import hijack_llama_attention\n",
    "    hijack_llama_attention()\n",
    "\n",
    "import autograd_4bit\n",
    "if ft_config.backend.lower() == 'triton':\n",
    "    autograd_4bit.switch_backend_to('triton')\n",
    "else:\n",
    "    autograd_4bit.switch_backend_to('cuda')\n",
    "```\n",
    "\n",
    "Instead it provide you a function like next:\n",
    "```python\n",
    "load_llama_model_4bit_low_ram, _, model_to_half, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=True,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")\n",
    "\n",
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "model_to_half(model)\n",
    "\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()\n",
    "```\n",
    "\n",
    "which run all the monkeypatching inside."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In progress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Init modules\n",
    "_, _, load_llama_model_4bit_low_ram, _, model_to_half, _, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=True,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")\n",
    "# Init model\n",
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "model_to_half(model)\n",
    "# CUDA amp wrapper\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()\n",
    "# Generation\n",
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                               do_sample=True,\n",
    "                               use_cache=False,\n",
    "                               repetition_penalty=1.1,\n",
    "                               max_new_tokens=128,\n",
    "                               temperature=0.9,\n",
    "                               top_p=0.95,\n",
    "                               top_k=40,\n",
    "                               return_dict_in_generate=True,\n",
    "                               output_attentions=False,\n",
    "                               output_hidden_states=False,\n",
    "                               output_scores=False)\n",
    "result_text = tokenizer.decode(generated['sequences'].cpu().tolist()[0])\n",
    "end = time.time()\n",
    "print(result_text)\n",
    "print(end - start)\n",
    "```\n",
    "It should give you something similar to this (tested it using Geforce 2080ti graphic card):\n",
    "```\n",
    "I think the meaning of life is to be happy, and that it is our birthright. That we are supposed to feel good as human beings, and that every moment of our lives should be a celebration of that.\n",
    "“I’m not saying that we always have to be happy, or that life always has to be perfect. But I do believe that we should strive to cultivate happiness and positivity in all areas of our lives, and that we should surround ourselves with people and things that bring us joy.\n",
    "“I believe that when we are surrounded by negativity, fear, and judgment, it can dull our light and keep\n",
    "27.77907705307007\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning example\n",
    "\n",
    "```python\n",
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "dataset = train_data.TrainTxt(\n",
    "    dataset=\"01_alpaca_text.txt\",\n",
    "    val_set_size=0,\n",
    "    tokenizer=tokenizer,\n",
    "    cutoff_len=256,\n",
    ")\n",
    "dataset.prepare_data(thd=-1, use_eos_token=1)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model = lora_model_zeros_and_scales_to_half(lora_model)\n",
    "\n",
    "apply_gradient_checkpointing(lora_model, checkpoint_ratio=1)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=5,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=None,\n",
    "    save_steps=50,\n",
    "    output_dir=\"lora-output-directory\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    train_dataset=dataset.train_data,\n",
    "    eval_dataset=dataset.val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "lora_model.config.use_cache = False\n",
    "\n",
    "trainer.train()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
