{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> The main module. Contains importer code as well as a simple inference test.\n",
    "> Tested on Geforce 2080Ti graphic card."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from flags import Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Matmul4BitOptions(Flags):\n",
    "    ACT_ORDER = 1\n",
    "    NO_ACT_ORDER = 2\n",
    "    ALGORYTHM_DEFAULT = 4\n",
    "    ALGORYTHM_OLDFASTER = 8\n",
    "    ALGORYTHM_FASTER = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_peft_tuners_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import replace_peft_model_with_int4_lora_model\n",
    "\n",
    "    replace_peft_model_with_int4_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_flash_attention_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n",
    "\n",
    "    replace_llama_attn_with_flash_attn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_xformers_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.llama_attn_hijack_xformers import hijack_llama_attention\n",
    "\n",
    "    hijack_llama_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _set_matmul_options(options: Matmul4BitOptions):\n",
    "    from alpaca_lora_4bit import matmul_utils_4bit\n",
    "    if Matmul4BitOptions.ACT_ORDER in options:\n",
    "        matmul_utils_4bit.act_order = True\n",
    "    elif Matmul4BitOptions.NO_ACT_ORDER in options:\n",
    "        matmul_utils_4bit.act_order = False\n",
    "    else:\n",
    "        raise ValueError(\"Need ACT_ORDER or NO_ACT_ORDER options\")\n",
    "    if Matmul4BitOptions.ALGORYTHM_DEFAULT in options:\n",
    "        matmul_utils_4bit.faster_mode = \"disabled\"\n",
    "    elif Matmul4BitOptions.ALGORYTHM_OLDFASTER in options:\n",
    "        assert not matmul_utils_4bit.act_order\n",
    "        matmul_utils_4bit.faster_mode = \"old_faster\"\n",
    "    elif Matmul4BitOptions.ALGORYTHM_FASTER in options:\n",
    "        assert not matmul_utils_4bit.act_order\n",
    "        matmul_utils_4bit.faster_mode = \"faster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def import_llama(\n",
    "        use_flash_attention: bool, # Use flash attention monkeypatch or not (shouldn't be used with use_xformerts)\n",
    "        use_xformers: bool, # Use xformers monkeypatch or not (shouldn't be used with use_flash_attention)\n",
    "        autograd_4bit_cuda: bool, # Use CUDA backend for 4bit stuff\n",
    "        autograd_4bit_triton: bool, # Use Triton backend for 4bit stuff\n",
    "        matmul4bit_options: Matmul4BitOptions = Matmul4BitOptions.NO_ACT_ORDER | Matmul4BitOptions.ALGORYTHM_DEFAULT,\n",
    "    ): #load_llama_model_4bit_low_ram / load_llama_model_4bit_low_ram_and_offload / model_to_half / model_to_float / Autograd4bitQuantLinear / AMPWrapper\n",
    "    \"\"\"\n",
    "    Do all the monkeypatching than return important objects of alpaca_lora_4bit library (arg_parser, train_data, load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, apply_gradient_checkpointing, Autograd4bitQuantLinear, AMPWrapper)\n",
    "    \"\"\"\n",
    "    _apply_peft_tuners_monkeypatch()\n",
    "    assert not (use_flash_attention and use_xformers)\n",
    "    if use_flash_attention:\n",
    "        _apply_flash_attention_monkeypatch()\n",
    "    if use_xformers:\n",
    "        _apply_xformers_monkeypatch()\n",
    "    from alpaca_lora_4bit import autograd_4bit\n",
    "    assert autograd_4bit_cuda ^ autograd_4bit_triton\n",
    "    if autograd_4bit_cuda:\n",
    "        autograd_4bit.switch_backend_to(\"cuda\")\n",
    "    if autograd_4bit_triton:\n",
    "        autograd_4bit.switch_backend_to(\"triton\")\n",
    "    _set_matmul_options(matmul4bit_options)\n",
    "    \n",
    "    from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, Autograd4bitQuantLinear, \\\n",
    "        model_to_half, model_to_float\n",
    "    from alpaca_lora_4bit.amp_wrapper import AMPWrapper\n",
    "    from alpaca_lora_4bit import train_data, arg_parser\n",
    "    from alpaca_lora_4bit.gradient_checkpointing import apply_gradient_checkpointing\n",
    "\n",
    "    return arg_parser, train_data, load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, apply_gradient_checkpointing, Autograd4bitQuantLinear, AMPWrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Here I will:\n",
    "\n",
    "- download model from [https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g](https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g)\n",
    "- initialize monkeypatches to use alpaca_lora_4bit including:\n",
    "    - flash attention\n",
    "    - triton\n",
    "- than initialize the model I have just downloaded\n",
    "- and do some simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../vicuna-13b-GPTQ-4bit-128g\"):\n",
    "    !git clone \"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\"\n",
    "    !mv \"vicuna-13b-GPTQ-4bit-128g\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm-4.65.0-py3.10.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\".\n",
      "Using CUDA implementation.\n",
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 26.82 seconds.\n",
      "Converted as Half.\n"
     ]
    }
   ],
   "source": [
    "_, _, load_llama_model_4bit_low_ram, _, model_to_half, _, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=False,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=True,\n",
    "    autograd_4bit_triton=False,\n",
    "    matmul4bit_options=Matmul4BitOptions.NO_ACT_ORDER | Matmul4BitOptions.ALGORYTHM_DEFAULT,\n",
    ")\n",
    "\n",
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "model_to_half(model)\n",
    "\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                               do_sample=True,\n",
    "                               use_cache=False,\n",
    "                               repetition_penalty=1.1,\n",
    "                               max_new_tokens=128,\n",
    "                               temperature=0.9,\n",
    "                               top_p=0.95,\n",
    "                               top_k=40,\n",
    "                               return_dict_in_generate=True,\n",
    "                               output_attentions=False,\n",
    "                               output_hidden_states=False,\n",
    "                               output_scores=False)\n",
    "result_text = tokenizer.decode(generated['sequences'].cpu().tolist()[0])\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the meaning of life is to love and be loved.”\n",
      "\n",
      "“But how about all the terrible things in the world?” I asked. “The wars, the cruelty, the injustice? How can you say that loving has anything to do with all that?”\n",
      "\n",
      "He looked at me very seriously. “I know it seems like a paradox,” he said.\n",
      "“But if people really loved each other, there would be no wars, no fighting, no cruelty, no injustice. Love is the only force that can change a person’s heart and make them want to do what’s right. If enough\n",
      "34.80921292304993\n"
     ]
    }
   ],
   "source": [
    "print(result_text)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
