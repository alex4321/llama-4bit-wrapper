{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> The main module. Contains importer code as well as a simple inference test.\n",
    "> Tested on Geforce 2080Ti graphic card."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports are only used for test purpose\n",
    "import time\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_peft_tuners_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import replace_peft_model_with_int4_lora_model\n",
    "\n",
    "    replace_peft_model_with_int4_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_flash_attention_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n",
    "\n",
    "    replace_llama_attn_with_flash_attn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_xformers_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.llama_attn_hijack_xformers import hijack_llama_attention\n",
    "\n",
    "    hijack_llama_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def import_llama(\n",
    "        use_flash_attention: bool, # Use flash attention monkeypatch or not (shouldn't be used with use_xformerts)\n",
    "        use_xformers: bool, # Use xformers monkeypatch or not (shouldn't be used with use_flash_attention)\n",
    "        autograd_4bit_cuda: bool, # Use CUDA backend for 4bit stuff\n",
    "        autograd_4bit_triton: bool # Use Triton backend for 4bit stuff\n",
    "    ): #load_llama_model_4bit_low_ram / load_llama_model_4bit_low_ram_and_offload / model_to_half / model_to_float / Autograd4bitQuantLinear / AMPWrapper\n",
    "    \"\"\"\n",
    "    Do all the monkeypatching than return important objects of alpaca_lora_4bit library (arg_parser, train_data, load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, apply_gradient_checkpointing, Autograd4bitQuantLinear, AMPWrapper)\n",
    "    \"\"\"\n",
    "    _apply_peft_tuners_monkeypatch()\n",
    "    assert not (use_flash_attention and use_xformers)\n",
    "    if use_flash_attention:\n",
    "        _apply_flash_attention_monkeypatch()\n",
    "    if use_xformers:\n",
    "        _apply_xformers_monkeypatch()\n",
    "    from alpaca_lora_4bit import autograd_4bit\n",
    "    assert autograd_4bit_cuda ^ autograd_4bit_triton\n",
    "    if autograd_4bit_cuda:\n",
    "        autograd_4bit.switch_backend_to(\"cuda\")\n",
    "    if autograd_4bit_triton:\n",
    "        autograd_4bit.switch_backend_to(\"triton\")\n",
    "    \n",
    "    from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, Autograd4bitQuantLinear, \\\n",
    "        model_to_half, model_to_float\n",
    "    from alpaca_lora_4bit.amp_wrapper import AMPWrapper\n",
    "    from alpaca_lora_4bit import train_data, arg_parser\n",
    "    from alpaca_lora_4bit.gradient_checkpointing import apply_gradient_checkpointing\n",
    "\n",
    "    return arg_parser, train_data, load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, apply_gradient_checkpointing, Autograd4bitQuantLinear, AMPWrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Here I will:\n",
    "\n",
    "- download model from [https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g](https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g)\n",
    "- initialize monkeypatches to use alpaca_lora_4bit including:\n",
    "    - flash attention\n",
    "    - triton\n",
    "- than initialize the model I have just downloaded\n",
    "- and do some simple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../vicuna-13b-GPTQ-4bit-128g\"):\n",
    "    !git clone \"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\"\n",
    "    !mv \"vicuna-13b-GPTQ-4bit-128g\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex4321/anaconda3/envs/longdocchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Triton implementation.\n",
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 3.24 seconds.\n",
      "Converted as Half.\n"
     ]
    }
   ],
   "source": [
    "_, _, load_llama_model_4bit_low_ram, _, model_to_half, _, _, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=True,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")\n",
    "\n",
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "model_to_half(model)\n",
    "\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                               do_sample=True,\n",
    "                               use_cache=False,\n",
    "                               repetition_penalty=1.1,\n",
    "                               max_new_tokens=128,\n",
    "                               temperature=0.9,\n",
    "                               top_p=0.95,\n",
    "                               top_k=40,\n",
    "                               return_dict_in_generate=True,\n",
    "                               output_attentions=False,\n",
    "                               output_hidden_states=False,\n",
    "                               output_scores=False)\n",
    "result_text = tokenizer.decode(generated['sequences'].cpu().tolist()[0])\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the meaning of life is to be happy, and that’s what we should all strive for. To me, it doesn’t matter how you achieve that happiness – whether it’s through religion, spirituality, family, friends, or just finding joy in everyday experiences – as long as you’re able to find it.\n",
      "“To thine own self be true.” This is a quote from Shakespeare’s Hamlet, and it means that you should always be honest with yourself and true to who you are. It’s important to be authentic and not try to be someone you’re not, because ultimately, you’ll be\n",
      "40.893144369125366\n"
     ]
    }
   ],
   "source": [
    "print(result_text)\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
