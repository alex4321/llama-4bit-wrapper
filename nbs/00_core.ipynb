{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> The main module. Contains importer code as well as a simple inference test.\n",
    "> Tested on Geforce 2080Ti graphic card."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports are only used for test purpose\n",
    "import time\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_peft_tuners_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import replace_peft_model_with_int4_lora_model\n",
    "\n",
    "    replace_peft_model_with_int4_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_flash_attention_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n",
    "\n",
    "    replace_llama_attn_with_flash_attn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_xformers_monkeypatch():\n",
    "    from alpaca_lora_4bit.monkeypatch.llama_attn_hijack_xformers import hijack_llama_attention\n",
    "\n",
    "    hijack_llama_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def import_alpaca(use_flash_attention: bool, use_xformers: bool, autograd_4bit_cuda: bool, autograd_4bit_triton: bool):\n",
    "    _apply_peft_tuners_monkeypatch()\n",
    "    assert not (use_flash_attention and use_xformers)\n",
    "    if use_flash_attention:\n",
    "        _apply_flash_attention_monkeypatch()\n",
    "    if use_xformers:\n",
    "        _apply_xformers_monkeypatch()\n",
    "    from alpaca_lora_4bit import autograd_4bit\n",
    "    assert autograd_4bit_cuda ^ autograd_4bit_triton\n",
    "    if autograd_4bit_cuda:\n",
    "        autograd_4bit.switch_backend_to(\"cuda\")\n",
    "    if autograd_4bit_triton:\n",
    "        autograd_4bit.switch_backend_to(\"triton\")\n",
    "    \n",
    "    from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, Autograd4bitQuantLinear, \\\n",
    "        model_to_half, model_to_float\n",
    "    from alpaca_lora_4bit.amp_wrapper import AMPWrapper\n",
    "\n",
    "    return load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, Autograd4bitQuantLinear, AMPWrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../vicuna-13b-GPTQ-4bit-128g\"):\n",
    "    !git clone \"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\"\n",
    "    !mv \"vicuna-13b-GPTQ-4bit-128g\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex4321/anaconda3/envs/longdocchat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Triton implementation.\n",
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 3.26 seconds.\n",
      "Converted as Half.\n"
     ]
    }
   ],
   "source": [
    "load_llama_model_4bit_low_ram, _, model_to_half, _, _, AMPWrapper = import_alpaca(\n",
    "    use_flash_attention=True,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=False,\n",
    "    autograd_4bit_triton=True,\n",
    ")\n",
    "\n",
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "model_to_half(model)\n",
    "\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''I think the meaning of life is'''\n",
    "batch = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "batch = {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(inputs=batch[\"input_ids\"],\n",
    "                               do_sample=True,\n",
    "                               use_cache=False,\n",
    "                               repetition_penalty=1.1,\n",
    "                               max_new_tokens=128,\n",
    "                               temperature=0.9,\n",
    "                               top_p=0.95,\n",
    "                               top_k=40,\n",
    "                               return_dict_in_generate=True,\n",
    "                               output_attentions=False,\n",
    "                               output_hidden_states=False,\n",
    "                               output_scores=False)\n",
    "result_text = tokenizer.decode(generated['sequences'].cpu().tolist()[0])\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the meaning of life is to be happy, and that it is our birthright. That we are supposed to feel good as human beings, and that every moment of our lives should be a celebration of that.\n",
      "“I’m not saying that we always have to be happy, or that life always has to be perfect. But I do believe that we should strive to cultivate happiness and positivity in all areas of our lives, and that we should surround ourselves with people and things that bring us joy.\n",
      "“I believe that when we are surrounded by negativity, fear, and judgment, it can dull our light and keep\n",
      "27.77907705307007\n"
     ]
    }
   ],
   "source": [
    "print(result_text)\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
