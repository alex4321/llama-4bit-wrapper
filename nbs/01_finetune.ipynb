{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "This module contains some functions useful to finetune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp finetune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm-4.65.0-py3.10.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports are only used in test\n",
    "from llama_4bit_wrapper import import_llama\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lora_model_zeros_and_scales_to_half(\n",
    "        model: PeftModel # Original model\n",
    "    ) -> PeftModel: # Converted model\n",
    "    \"\"\"\n",
    "    Convert zeros and scales for PeftModel to half-precision\n",
    "    \"\"\"\n",
    "    for _, m in model.named_modules():\n",
    "        if \"Autograd4bitQuantLinear\" in str(type(m)) or \"Linear4bitLt\" in str(type(m)):\n",
    "            if hasattr(m, \"is_v1_model\") and m.is_v1_model:\n",
    "                m.zeros = m.zeros.half()\n",
    "            m.scales = m.scales.half()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a *very* simple test here I will just try to:\n",
    "\n",
    "- see the perplexity of pretrained Vicuna model for the sample text file\n",
    "- train the LoRA adapters on top of Vicuna\n",
    "- see the perplexity of such a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\".\n",
      "Using CUDA implementation.\n"
     ]
    }
   ],
   "source": [
    "_, train_data, load_llama_model_4bit_low_ram, _, model_to_half, _, apply_gradient_checkpointing, _, AMPWrapper = import_llama(\n",
    "    use_flash_attention=False,\n",
    "    use_xformers=False,\n",
    "    autograd_4bit_cuda=True,\n",
    "    autograd_4bit_triton=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../vicuna-13b-GPTQ-4bit-128g\"):\n",
    "    !git clone \"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g\"\n",
    "    !mv \"vicuna-13b-GPTQ-4bit-128g\" .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 6.77 seconds.\n"
     ]
    }
   ],
   "source": [
    "model_pretrained, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 0.00% outliers\n"
     ]
    }
   ],
   "source": [
    "dataset = train_data.TrainTxt(\n",
    "    dataset=\"01_alpaca_text.txt\",\n",
    "    val_set_size=0,\n",
    "    tokenizer=tokenizer,\n",
    "    cutoff_len=256,\n",
    ")\n",
    "dataset.prepare_data(thd=-1, use_eos_token=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted as Half.\n"
     ]
    }
   ],
   "source": [
    "model_to_half(model_pretrained)\n",
    "model_pretrained_wrapper = AMPWrapper(model_pretrained)\n",
    "model_pretrained_wrapper.apply_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_model(model, data):\n",
    "    probabilities = []\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            input_ids = torch.LongTensor([sample[\"input_ids\"]]).cuda()\n",
    "            response = model.forward(input_ids, return_dict=True)\n",
    "            logits = response['logits'][0]\n",
    "            probas = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            proba = probas.max(dim=-1).values.mean().item()\n",
    "            probabilities.append(proba)\n",
    "    average_proba = sum(probabilities) / len(probabilities)\n",
    "    return average_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7032277960526315"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pretrained.eval()\n",
    "_test_model(model_pretrained, dataset.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 5.54 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\n",
    "    config_path=\"../vicuna-13b-GPTQ-4bit-128g/\",\n",
    "    model_path=\"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "    groupsize=128,\n",
    "    is_v1_model=False,\n",
    ")\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model = lora_model_zeros_and_scales_to_half(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Patch Applied For Block 0\n",
      "Forward Patch Applied For Block 1\n",
      "Forward Patch Applied For Block 2\n",
      "Forward Patch Applied For Block 3\n",
      "Forward Patch Applied For Block 4\n",
      "Forward Patch Applied For Block 5\n",
      "Forward Patch Applied For Block 6\n",
      "Forward Patch Applied For Block 7\n",
      "Forward Patch Applied For Block 8\n",
      "Forward Patch Applied For Block 9\n",
      "Forward Patch Applied For Block 10\n",
      "Forward Patch Applied For Block 11\n",
      "Forward Patch Applied For Block 12\n",
      "Forward Patch Applied For Block 13\n",
      "Forward Patch Applied For Block 14\n",
      "Forward Patch Applied For Block 15\n",
      "Forward Patch Applied For Block 16\n",
      "Forward Patch Applied For Block 17\n",
      "Forward Patch Applied For Block 18\n",
      "Forward Patch Applied For Block 19\n",
      "Forward Patch Applied For Block 20\n",
      "Forward Patch Applied For Block 21\n",
      "Forward Patch Applied For Block 22\n",
      "Forward Patch Applied For Block 23\n",
      "Forward Patch Applied For Block 24\n",
      "Forward Patch Applied For Block 25\n",
      "Forward Patch Applied For Block 26\n",
      "Forward Patch Applied For Block 27\n",
      "Forward Patch Applied For Block 28\n",
      "Forward Patch Applied For Block 29\n",
      "Forward Patch Applied For Block 30\n",
      "Forward Patch Applied For Block 31\n",
      "Forward Patch Applied For Block 32\n",
      "Forward Patch Applied For Block 33\n",
      "Forward Patch Applied For Block 34\n",
      "Forward Patch Applied For Block 35\n",
      "Forward Patch Applied For Block 36\n",
      "Forward Patch Applied For Block 37\n",
      "Forward Patch Applied For Block 38\n",
      "Forward Patch Applied For Block 39\n",
      "Var Wrapper Patch Applied\n"
     ]
    }
   ],
   "source": [
    "apply_gradient_checkpointing(lora_model, checkpoint_ratio=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=5,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=None,\n",
    "    save_steps=50,\n",
    "    output_dir=\"lora-output-directory\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    train_dataset=dataset.train_data,\n",
    "    eval_dataset=dataset.val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "lora_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5157, 'learning_rate': 0.0002925663716814159, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9429, 'learning_rate': 0.0002830088495575221, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2259, 'learning_rate': 0.00027238938053097343, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0958, 'learning_rate': 0.00026176991150442475, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4413, 'learning_rate': 0.00025115044247787607, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5469, 'learning_rate': 0.00024053097345132742, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2576, 'learning_rate': 0.00022991150442477874, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1367, 'learning_rate': 0.00021929203539823008, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9065, 'learning_rate': 0.00020867256637168138, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6604, 'learning_rate': 0.00019805309734513272, 'epoch': 3.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7005, 'learning_rate': 0.00018743362831858404, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4831, 'learning_rate': 0.0001768141592920354, 'epoch': 4.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3282, 'learning_rate': 0.00016619469026548668, 'epoch': 4.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5085, 'learning_rate': 0.00015557522123893803, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2686, 'learning_rate': 0.00014495575221238938, 'epoch': 5.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2448, 'learning_rate': 0.0001343362831858407, 'epoch': 5.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2158, 'learning_rate': 0.00012371681415929202, 'epoch': 5.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2495, 'learning_rate': 0.00011309734513274335, 'epoch': 6.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2347, 'learning_rate': 0.00010247787610619469, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.124, 'learning_rate': 9.185840707964601e-05, 'epoch': 7.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1026, 'learning_rate': 8.123893805309734e-05, 'epoch': 7.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0842, 'learning_rate': 7.061946902654867e-05, 'epoch': 7.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3344, 'learning_rate': 5.9999999999999995e-05, 'epoch': 8.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1178, 'learning_rate': 4.938053097345133e-05, 'epoch': 8.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1294, 'learning_rate': 3.8761061946902655e-05, 'epoch': 8.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1697, 'learning_rate': 2.814159292035398e-05, 'epoch': 9.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1052, 'learning_rate': 1.752212389380531e-05, 'epoch': 9.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2013, 'learning_rate': 6.9026548672566364e-06, 'epoch': 9.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1937.9553, 'train_samples_per_second': 0.294, 'train_steps_per_second': 0.294, 'train_loss': 0.7503459926237139, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 570/570 [32:17<00:00,  3.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=570, training_loss=0.7503459926237139, metrics={'train_runtime': 1937.9553, 'train_samples_per_second': 0.294, 'train_steps_per_second': 0.294, 'train_loss': 0.7503459926237139, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9679228182424578"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.eval()\n",
    "_test_model(lora_model, dataset.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
