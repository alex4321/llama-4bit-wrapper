# Autogenerated by nbdev

d = { 'settings': { 'branch': 'main',
                'doc_baseurl': '/llama-4bit-wrapper',
                'doc_host': 'https://alex4321.github.io',
                'git_url': 'https://github.com/alex4321/llama-4bit-wrapper',
                'lib_path': 'llama_4bit_wrapper'},
  'syms': { 'llama_4bit_wrapper.core': { 'llama_4bit_wrapper.core._apply_flash_attention_monkeypatch': ( 'core.html#_apply_flash_attention_monkeypatch',
                                                                                                         'llama_4bit_wrapper/core.py'),
                                         'llama_4bit_wrapper.core._apply_peft_tuners_monkeypatch': ( 'core.html#_apply_peft_tuners_monkeypatch',
                                                                                                     'llama_4bit_wrapper/core.py'),
                                         'llama_4bit_wrapper.core._apply_xformers_monkeypatch': ( 'core.html#_apply_xformers_monkeypatch',
                                                                                                  'llama_4bit_wrapper/core.py'),
                                         'llama_4bit_wrapper.core.import_llama': ('core.html#import_llama', 'llama_4bit_wrapper/core.py')},
            'llama_4bit_wrapper.finetune': { 'llama_4bit_wrapper.finetune.lora_model_zeros_and_scales_to_half': ( 'finetune.html#lora_model_zeros_and_scales_to_half',
                                                                                                                  'llama_4bit_wrapper/finetune.py')}}}
