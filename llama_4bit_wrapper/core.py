# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['Matmul4BitOptions', 'import_llama']

# %% ../nbs/00_core.ipynb 3
import time
import torch
import os
from flags import Flags

# %% ../nbs/00_core.ipynb 4
class Matmul4BitOptions(Flags):
    ACT_ORDER = 1
    NO_ACT_ORDER = 2
    ALGORYTHM_DEFAULT = 4
    ALGORYTHM_OLDFASTER = 8
    ALGORYTHM_FASTER = 16

# %% ../nbs/00_core.ipynb 5
def _apply_peft_tuners_monkeypatch():
    from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import replace_peft_model_with_int4_lora_model

    replace_peft_model_with_int4_lora_model()

# %% ../nbs/00_core.ipynb 6
def _apply_flash_attention_monkeypatch():
    from alpaca_lora_4bit.monkeypatch.llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn

    replace_llama_attn_with_flash_attn()

# %% ../nbs/00_core.ipynb 7
def _apply_xformers_monkeypatch():
    from alpaca_lora_4bit.monkeypatch.llama_attn_hijack_xformers import hijack_llama_attention

    hijack_llama_attention()

# %% ../nbs/00_core.ipynb 8
def _set_matmul_options(options: Matmul4BitOptions):
    from alpaca_lora_4bit import matmul_utils_4bit
    if Matmul4BitOptions.ACT_ORDER in options:
        matmul_utils_4bit.act_order = True
    elif Matmul4BitOptions.NO_ACT_ORDER in options:
        matmul_utils_4bit.act_order = False
    else:
        raise ValueError("Need ACT_ORDER or NO_ACT_ORDER options")
    if Matmul4BitOptions.ALGORYTHM_DEFAULT in options:
        matmul_utils_4bit.faster_mode = "disabled"
    elif Matmul4BitOptions.ALGORYTHM_OLDFASTER in options:
        assert not matmul_utils_4bit.act_order
        matmul_utils_4bit.faster_mode = "old_faster"
    elif Matmul4BitOptions.ALGORYTHM_FASTER in options:
        assert not matmul_utils_4bit.act_order
        matmul_utils_4bit.faster_mode = "faster"

# %% ../nbs/00_core.ipynb 9
def import_llama(
        use_flash_attention: bool, # Use flash attention monkeypatch or not (shouldn't be used with use_xformerts)
        use_xformers: bool, # Use xformers monkeypatch or not (shouldn't be used with use_flash_attention)
        autograd_4bit_cuda: bool, # Use CUDA backend for 4bit stuff
        autograd_4bit_triton: bool, # Use Triton backend for 4bit stuff
        matmul4bit_options: Matmul4BitOptions = Matmul4BitOptions.NO_ACT_ORDER | Matmul4BitOptions.ALGORYTHM_DEFAULT,
    ): #load_llama_model_4bit_low_ram / load_llama_model_4bit_low_ram_and_offload / model_to_half / model_to_float / Autograd4bitQuantLinear / AMPWrapper
    """
    Do all the monkeypatching than return important objects of alpaca_lora_4bit library (arg_parser, train_data, load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, apply_gradient_checkpointing, Autograd4bitQuantLinear, AMPWrapper)
    """
    _apply_peft_tuners_monkeypatch()
    assert not (use_flash_attention and use_xformers)
    if use_flash_attention:
        _apply_flash_attention_monkeypatch()
    if use_xformers:
        _apply_xformers_monkeypatch()
    from alpaca_lora_4bit import autograd_4bit
    assert autograd_4bit_cuda ^ autograd_4bit_triton
    if autograd_4bit_cuda:
        autograd_4bit.switch_backend_to("cuda")
    if autograd_4bit_triton:
        autograd_4bit.switch_backend_to("triton")
    _set_matmul_options(matmul4bit_options)
    
    from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, Autograd4bitQuantLinear, \
        model_to_half, model_to_float
    from alpaca_lora_4bit.amp_wrapper import AMPWrapper
    from alpaca_lora_4bit import train_data, arg_parser
    from alpaca_lora_4bit.gradient_checkpointing import apply_gradient_checkpointing

    return arg_parser, train_data, load_llama_model_4bit_low_ram, load_llama_model_4bit_low_ram_and_offload, model_to_half, model_to_float, apply_gradient_checkpointing, Autograd4bitQuantLinear, AMPWrapper
